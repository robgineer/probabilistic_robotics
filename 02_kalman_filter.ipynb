{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7644cd-ab32-452d-a87d-a386d558b57c",
   "metadata": {},
   "source": [
    "# The Kalman Filter\n",
    "\n",
    "introduced in 1958, is one of the most used filters to process noisy data.\n",
    "\n",
    "In order to be able to understand its functionality, the following text shows an overview of the theory and the required computations. Since this notebook aims to give a very small and basic overview, we will not focus on the proofs of the Kalman Filter math. For more detailed explanations with deeper insights on the Kalman Filter kindly refer to [[1]](#Thrun) and [[2]](#kf_notebook).\n",
    "\n",
    "\n",
    "The basic functionality of the Kalman Filter is quite simple as we have only two stages:\n",
    "1. Predict and\n",
    "2. Update\n",
    "\n",
    "\n",
    "Within the predict part, we estimate a potential position using a motion model and whithin the update part, we take in the observations, identify how confident these are based on our expected estimate from the predictions part and finally update the estimate.\n",
    "\n",
    "The Kalman Filter is often mentioned to be a \"Gaussian Filter\"; which basically says the following:\n",
    "\n",
    "1. Our inputs are Gaussian distributed\n",
    "2. All computations within the Kalman Filter preseve the Gaussian distribution\n",
    "   \n",
    "The second describes that adding and multiplying two Gaussians $\\mathcal{N}(\\mu_1,\\,\\sigma_1^{2})$ and $\\mathcal{N}(\\mu_2,\\,\\sigma_2^{2})$, is based on the following rules.\n",
    "\n",
    "**Addition** \n",
    "$$\n",
    "\\mu = \\mu_1 + \\mu_2 \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^{2} = \\sigma_1^{2} + \\sigma_2^{2} \n",
    "$$\n",
    "\n",
    "\n",
    "**Multiplication**\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{\\mu_1 \\sigma_2^{2} \\ + \\mu_2 \\sigma_1^{2}}{\\sigma_1^{2} + \\sigma_2^{2}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^{2} = \\frac{\\sigma_1^{2} \\sigma_2^{2}}{\\sigma_1^{2} + \\sigma_2^{2}} \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967044c-9b1c-43fe-8be4-5ba5c040c654",
   "metadata": {},
   "source": [
    "## Kalman Filter Theory\n",
    "\n",
    "Let's go trough some theory first.\n",
    "\n",
    "As previously mentioned, the Kalman Filter contains two steps: **predict** and **update**. We will start with an overview of the first followed by the latter.\n",
    "\n",
    "The values required to be calculated within both steps are\n",
    "\n",
    "```\n",
    "Predict\n",
    "x: state estimate (mean of Gaussian)\n",
    "P: state estimate covariance\n",
    "Q: process covariance matrix \n",
    "B: control input model\n",
    "u: control input\n",
    "F: state transition matrix\n",
    "\n",
    "Update\n",
    "z: measurement\n",
    "H: measurement function\n",
    "R: measurement noise\n",
    "y: error\n",
    "S: innovation\n",
    "K: Kalman Gain\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f99665-328f-477a-8617-1781502ec7e4",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "Out state vector **x** consists of two elements, the actual $x$ position and the velocity $v$, that represents the hidden or latent variable as it is something we do not measure but derive from the measurements.\n",
    "$$\n",
    "\\bf{x} = \\begin{bmatrix} x \\\\ v \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Yes, the `state` $\\bf{x}$ and the `observation` $x$ are (unfortunately) represented by the same variable (while the state is expressed in bold).\n",
    "\n",
    "The `state covariance` is the variance of a Gaussian distribution in multidimensional space. In simple words: it represents the confidence of an estimated state (a higher covariance implies a greater spread).\n",
    "This matrix is constantly updated but requires initialisation. We will set the $x$ position covariance to $10$ and the velocity to $5$, implying\n",
    "$$\n",
    "P = \\begin{bmatrix} 10 & 0 \\\\ 0 & 5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The `process covariance matrix` $\\bf{Q}$ represents the uncertainty of our prediction model. Since we expect that our prediction can never be 100% accurate, we can model the deviation using this matrix. $\\bf{Q}$ has also a very practical benefit: it enables the `state covariance` $\\bf{P}$ to be positive definite ($\\neq 0$). A zero state covariance simply states that we are too confident with our predictions (hence, no covariance), which in general is a bit too optimistic. It moreover implies that the `innovation` (explained below) would become a zero matrix as well (making it impossible to calculate its inverse). \n",
    "\n",
    "We will set $\\bf{Q}$ to\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\bf{Q} = \\begin{bmatrix} 0.05 \\\\ 0.05 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "The `control input` and the `control model` ($\\bf{u}$ and $\\bf{B}$) aim to define the uncertainty resulting from a control system. In our case this would be subject to the vehicles electrical / mechanical controllers that transform a digital signal (for steering, acceleration / deceleration) into mechanical motion. We will omit modelling the control system uncertainty (as it is subject to very detailed Kalman Filter design requiring knowlegde of the control system such as the power train system used).\n",
    "\n",
    "<br/>\n",
    "\n",
    "The `transition matrix` $\\bf{F}$ is the interesting one. It defines how states are predicted at step $i$ using the previous state estimate.\n",
    "\n",
    "It could become arbitray complex and in our case we will keep it a bit simple by using the **constant velocity motion model** that implies a constant velocity between two observation steps. In other words: we will expect an object to travel with the same speed in between two actual observations. Refer `01_motion_models` notebook for more details on the model used.\n",
    "Hence, we define $\\bf{F}$ as \n",
    "$$\n",
    "\\bf{F} = \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, for the **prediction step**, we need to estimate our state as well as our covariance. \n",
    "\n",
    "For the state estimation at at prediction step $i$ we multiply the transition matrix $F$ with the previous state $\\bf{x_{i-1}}$ and add the control input / model to it, yielding\n",
    "\n",
    "$$\n",
    "\\bf{x_i} = F \\cdot \\bf{x_{i-1}} + B \\cdot u\n",
    "$$\n",
    "\n",
    "Since we omit the control specific parts, we will define our state estimation as\n",
    "\n",
    "$$\n",
    "\\bf{x_i} = F \\cdot \\bf{x_{i-1}}\n",
    "$$\n",
    "\n",
    "One may ask \"how this could represent a **linear motion model**\". The answer reveals itself by having a closer look at the resulting calculations\n",
    "\n",
    "$$\n",
    "\\bf{x_i} =  F \\cdot \\bf{x_{i-1}}\n",
    "= \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} x_{i-1} \\\\ v \\end{bmatrix}\n",
    "=  \\begin{bmatrix} 1 \\cdot x_{i-1}  + \\Delta t \\cdot v_{i-1}  \\\\ 0 \\cdot x_{i-1}  + 1 \\cdot v_{i-1} \\end{bmatrix} = \\begin{bmatrix} x_{i-1}  + \\Delta t \\cdot v_{i-1} \\\\ v_{i-1} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "=> for the new position in $x$ direction, we would use $x_{i-1} + \\Delta t \\cdot v_{i-1}$, which defines a *linear* motion and for the velocity, we would just keep the old value (as we are expecting it to be constant).\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "For the covariance part, we use\n",
    "$$\n",
    "\\bf{P} = \\bf{F} \\cdot \\bf{P} \\cdot \\bf{Fˆ{T}} + \\bf{Q}\n",
    "$$\n",
    "\n",
    "... which looks rather cryptic. Let's have a closer look on that one as well using the intial values defined for $\\bf{P}$ above (omitting $\\bf{Q}$).\n",
    "\n",
    "$$\n",
    "\\bf{P_i} = \\bf{F} \\cdot \\bf{P_{i-1}} \\cdot \\bf{Fˆ{T}}\n",
    "= \\begin{bmatrix} 1 & \\Delta t \\\\ 0 & 1 \\end{bmatrix} \\cdot \\begin{bmatrix} 10 & 0 \\\\ 0 & 5 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 0 \\\\ \\Delta t & 1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 10 & 5 \\Delta t \\\\ 0 & 5 \\end{bmatrix} \\cdot \\begin{bmatrix} 1 & 0 \\\\ \\Delta t & 1 \\end{bmatrix}\n",
    "= \\begin{bmatrix} 10 + 5 \\Delta tˆ2 & 5 \\Delta t \\\\ 5 \\Delta t & 5\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can see that the resulting matrix\n",
    "$$\n",
    "\\begin{bmatrix} 10 + 5 \\Delta tˆ2 & 5 \\Delta t \\\\ 5 \\Delta t & 5\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "shows is a correlation between the observable variable (the position) and the latent variable (the velocity). Although we have initially defined that both are uncorrelated (refer the initialisation of $\\bf{P}$), we can see that both variances are affecting each other.\n",
    "\n",
    "A more detailed explanation on this particular topic is presented in [[2]](#kf_notebook).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d3b5e-598d-4112-a55d-33655ee87f8a",
   "metadata": {},
   "source": [
    "### Update\n",
    "\n",
    "The `measurement` we receive in our 1D case is the position in $x$ direction. Hence, the measurement is a 1D vector\n",
    "\n",
    "$$\n",
    "z = \\begin{bmatrix} x \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The `measurement function` defines which states are hidden and which ones are actually observed. In our case the position $x$ is the variable we observe and velocity $v$ is the one we derive from the measurements. Hence, our measurement function is\n",
    "$$\n",
    "\\bf{H} = \\begin{bmatrix} 1 & 0 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "The `measurement noise` $\\bf{R}$ defines the uncertainty of the measurements and is something we would need to get from the sensor manufacturer. Since this is an information we do not have for or dataset, we set the measurement noise for our sensor to\n",
    "$$\n",
    "\\bf{R} = 0.05\n",
    "$$\n",
    "\n",
    "<br/>\n",
    "\n",
    "Now, for the `update` step have several equations. We first, need to calculate an error representing the difference between the predicted and the observed value \n",
    "$$\n",
    "y = z - \\bf{H} \\cdot \\bf{x}\n",
    "$$\n",
    "\n",
    "... which is straighforward to understand as it is literally just the differce between the estimated state in \"measurement space\" (= we do not consider the velocity for measurement).\n",
    "\n",
    "Next ist to calculate the `innovation` $\\bf{S}$ \n",
    "\n",
    "$$\n",
    "\\bf{S} = \\bf{H} \\cdot \\bf{P} \\cdot \\bf{Hˆ{T}} + \\bf{R}\n",
    "$$\n",
    "\n",
    "The purpose of $\\bf{S}$ is to \"project the covariance matrix into measurement space\" [2].\n",
    "\n",
    "<br/>\n",
    "\n",
    "Followed by the `Kalman Gain`:\n",
    "\n",
    "$$\n",
    "\\bf{K} = \\bf{P} \\cdot \\bf{Hˆ{T}} \\cdot \\bf{S}ˆ{-1} \n",
    "$$\n",
    "\n",
    "that \"specifies the degree to which the measurement is incoprated into the new state estimate\" [[1]](#Thrun) (and is very similar to a weight).\n",
    "\n",
    "<br/>\n",
    "\n",
    "And we finally got to the update of our state estimation as well as the covariance at step $i$\n",
    "\n",
    "$$\n",
    "\\bf{x_i} = \\bf{x_{i-1}} + \\bf{K} \\cdot \\bf{y}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bf{P_i} = (\\bf{I} - \\bf{K} \\cdot \\bf{H}) \\cdot \\bf{P_{i-1}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a7f5a-bd87-4ae7-8c25-e4a6a3bff6f0",
   "metadata": {},
   "source": [
    "## Kalman Filter Implementation\n",
    "\n",
    "Once the matrices above are clear, the implementation is rather simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96b5fe17-4a65-42d8-9323-b648fd4c6c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import inv\n",
    "\n",
    "class KalmanFilter(object):\n",
    "    def __init__(self, P, F, R, Q, H, init_state = np.zeros((2)), num_states = 2):\n",
    "        self.P = P\n",
    "        self.F = F\n",
    "        self.R = R\n",
    "        self.Q = Q\n",
    "        self.H = H\n",
    "        self.num_states = num_states\n",
    "        self.x = init_state\n",
    "\n",
    "    def predict(self):\n",
    "        self.x = self.F @ self.x\n",
    "        self.P = self.F @ self.P @ self.F.T + self.Q\n",
    "        \n",
    "        \n",
    "    def update(self, z):\n",
    "        # error\n",
    "        y = z - self.H @ self.x\n",
    "        # innovation\n",
    "        S = self.H @ self.P @ self.H.T + self.R\n",
    "        # Kalman gain\n",
    "        K = self.P @ self.H.T @ inv(S)\n",
    "        # update state and covariance\n",
    "        self.x = self.x + (K @ y)\n",
    "        self.P = (np.eye(self.num_states, self.num_states) - K @ self.H) @ self.P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0356457-7019-411c-b238-1150f3167ff9",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\n",
    "<a id=\"Thrun\">[1]</a>: Thrun, Burgard, Fox, Probabilistic Robotics, MIT Press, 2006\n",
    "\n",
    "<a id=\"kf_notebook\">[2]</a>: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/master/06-Multivariate-Kalman-Filters.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a202656c-f9d2-4a1a-acab-d072be9656ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
